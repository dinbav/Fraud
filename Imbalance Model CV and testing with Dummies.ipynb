{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Imbalance Model CV and testing with Dummies.ipynb","provenance":[{"file_id":"1KF3dTxW36ZUAikiHkAZoaQp4bWMuqQ-G","timestamp":1577352170257},{"file_id":"1K8Cdsjf5qlQ4lofW-3z3cgmVkBa19eI2","timestamp":1576566999354},{"file_id":"1DO6TgFudhqIWxrCrsM8hiH3-ECxLYSHv","timestamp":1576500909847}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"3BlOvB02_AiA","colab_type":"code","colab":{}},"source":["#pip install --upgrade scikit-learn"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tR-AlbgeTeOQ","colab_type":"code","colab":{}},"source":["from sklearn.feature_selection import mutual_info_classif\n","import pandas as pd\n","import numpy as np\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import chi2\n","from scipy.stats import chisquare\n","from sklearn import datasets, linear_model\n","from sklearn.model_selection import cross_validate\n","from sklearn.metrics import make_scorer\n","from sklearn.metrics import confusion_matrix\n","from sklearn.svm import LinearSVC\n","import csv\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import cross_validate\n","from sklearn.metrics import recall_score"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OUBAgyTK5-au","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hpux42UG8WWw","colab_type":"code","colab":{}},"source":["from sklearn.metrics import f1_score"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_i_nG5EtA56P","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn import metrics\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import classification_report\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PxIdjtwr2Uv3","colab_type":"code","outputId":"bd739388-c378-4d99-a646-5eeee21fee3c","executionInfo":{"status":"ok","timestamp":1577613497554,"user_tz":-120,"elapsed":535,"user":{"displayName":"din bav","photoUrl":"","userId":"06942881549123238413"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["from imblearn.over_sampling import SMOTE"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n","  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"l9lfA9AdUcRq","colab_type":"code","outputId":"246d4f4c-d066-4e25-b3f4-262d65175dca","executionInfo":{"status":"ok","timestamp":1577613565195,"user_tz":-120,"elapsed":766,"user":{"displayName":"din bav","photoUrl":"","userId":"06942881549123238413"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["from google.colab import drive \n","drive.mount('/content/gdrive')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pHZF87oDp17s","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iyFx_SQltpLA","colab_type":"code","colab":{}},"source":["pd.set_option('display.max_rows', 500)\n","pd.set_option('display.max_columns', 500)\n","pd.set_option('display.width', 1000)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wasFAtfvTxAf","colab_type":"code","colab":{}},"source":["d=pd.read_csv('gdrive/My Drive/Colab Notebooks/Predictive Modeling/Fraud/DATA/3. fraud_remove_nulls_fill_random_distribution/test-fraud_remove_nulls_fill_random_distribution.csv',compression='gzip')#2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oacqOzbKqo0n","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":217},"outputId":"b861c51d-7d38-461d-96b3-b743818c3388","executionInfo":{"status":"ok","timestamp":1577613512057,"user_tz":-120,"elapsed":11151,"user":{"displayName":"din bav","photoUrl":"","userId":"06942881549123238413"}}},"source":["d.head()"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>isFraud</th>\n","      <th>TransactionAmt</th>\n","      <th>ProductCD</th>\n","      <th>card1</th>\n","      <th>card2</th>\n","      <th>card3</th>\n","      <th>card4</th>\n","      <th>card5</th>\n","      <th>card6</th>\n","      <th>addr1</th>\n","      <th>addr2</th>\n","      <th>dist1</th>\n","      <th>C1</th>\n","      <th>C2</th>\n","      <th>C3</th>\n","      <th>C4</th>\n","      <th>C5</th>\n","      <th>C6</th>\n","      <th>C7</th>\n","      <th>C8</th>\n","      <th>C9</th>\n","      <th>C10</th>\n","      <th>C11</th>\n","      <th>C12</th>\n","      <th>C13</th>\n","      <th>C14</th>\n","      <th>D1</th>\n","      <th>D2</th>\n","      <th>D3</th>\n","      <th>D4</th>\n","      <th>D5</th>\n","      <th>D10</th>\n","      <th>D11</th>\n","      <th>D15</th>\n","      <th>M1</th>\n","      <th>M2</th>\n","      <th>M3</th>\n","      <th>M4</th>\n","      <th>M5</th>\n","      <th>M6</th>\n","      <th>M7</th>\n","      <th>M8</th>\n","      <th>M9</th>\n","      <th>V1</th>\n","      <th>V2</th>\n","      <th>V3</th>\n","      <th>V4</th>\n","      <th>V5</th>\n","      <th>V6</th>\n","      <th>V7</th>\n","      <th>V8</th>\n","      <th>V9</th>\n","      <th>V10</th>\n","      <th>V11</th>\n","      <th>V12</th>\n","      <th>V13</th>\n","      <th>V14</th>\n","      <th>V15</th>\n","      <th>V16</th>\n","      <th>V17</th>\n","      <th>V18</th>\n","      <th>V19</th>\n","      <th>V20</th>\n","      <th>V21</th>\n","      <th>V22</th>\n","      <th>V23</th>\n","      <th>V24</th>\n","      <th>V25</th>\n","      <th>V26</th>\n","      <th>V27</th>\n","      <th>V28</th>\n","      <th>V29</th>\n","      <th>V30</th>\n","      <th>V31</th>\n","      <th>V32</th>\n","      <th>V33</th>\n","      <th>V34</th>\n","      <th>V35</th>\n","      <th>V36</th>\n","      <th>V37</th>\n","      <th>V38</th>\n","      <th>V39</th>\n","      <th>V40</th>\n","      <th>V41</th>\n","      <th>V42</th>\n","      <th>V43</th>\n","      <th>V44</th>\n","      <th>V45</th>\n","      <th>V46</th>\n","      <th>V47</th>\n","      <th>V48</th>\n","      <th>V49</th>\n","      <th>V50</th>\n","      <th>V51</th>\n","      <th>V52</th>\n","      <th>V53</th>\n","      <th>V54</th>\n","      <th>V55</th>\n","      <th>V56</th>\n","      <th>V57</th>\n","      <th>V58</th>\n","      <th>V59</th>\n","      <th>V60</th>\n","      <th>V61</th>\n","      <th>V62</th>\n","      <th>V63</th>\n","      <th>V64</th>\n","      <th>V65</th>\n","      <th>V66</th>\n","      <th>V67</th>\n","      <th>V68</th>\n","      <th>V69</th>\n","      <th>V70</th>\n","      <th>V71</th>\n","      <th>V72</th>\n","      <th>V73</th>\n","      <th>V74</th>\n","      <th>V75</th>\n","      <th>V76</th>\n","      <th>V77</th>\n","      <th>V78</th>\n","      <th>V79</th>\n","      <th>V80</th>\n","      <th>V81</th>\n","      <th>V82</th>\n","      <th>V83</th>\n","      <th>V84</th>\n","      <th>V85</th>\n","      <th>V86</th>\n","      <th>V87</th>\n","      <th>V88</th>\n","      <th>V89</th>\n","      <th>V90</th>\n","      <th>V91</th>\n","      <th>V92</th>\n","      <th>V93</th>\n","      <th>V94</th>\n","      <th>V95</th>\n","      <th>V96</th>\n","      <th>V97</th>\n","      <th>V98</th>\n","      <th>V99</th>\n","      <th>V100</th>\n","      <th>V101</th>\n","      <th>V102</th>\n","      <th>V103</th>\n","      <th>V104</th>\n","      <th>V105</th>\n","      <th>V106</th>\n","      <th>V107</th>\n","      <th>V108</th>\n","      <th>V109</th>\n","      <th>V110</th>\n","      <th>V111</th>\n","      <th>V112</th>\n","      <th>V113</th>\n","      <th>V114</th>\n","      <th>V115</th>\n","      <th>V116</th>\n","      <th>V117</th>\n","      <th>V118</th>\n","      <th>V119</th>\n","      <th>V120</th>\n","      <th>V121</th>\n","      <th>V122</th>\n","      <th>V123</th>\n","      <th>V124</th>\n","      <th>V125</th>\n","      <th>V126</th>\n","      <th>V127</th>\n","      <th>V128</th>\n","      <th>V129</th>\n","      <th>V130</th>\n","      <th>V131</th>\n","      <th>V132</th>\n","      <th>V133</th>\n","      <th>V134</th>\n","      <th>V135</th>\n","      <th>V136</th>\n","      <th>V137</th>\n","      <th>V167</th>\n","      <th>V168</th>\n","      <th>V169</th>\n","      <th>V170</th>\n","      <th>V171</th>\n","      <th>V172</th>\n","      <th>V173</th>\n","      <th>V174</th>\n","      <th>V175</th>\n","      <th>V176</th>\n","      <th>V177</th>\n","      <th>V178</th>\n","      <th>V179</th>\n","      <th>V180</th>\n","      <th>V181</th>\n","      <th>V182</th>\n","      <th>V183</th>\n","      <th>V184</th>\n","      <th>V185</th>\n","      <th>V186</th>\n","      <th>V187</th>\n","      <th>V188</th>\n","      <th>V189</th>\n","      <th>V190</th>\n","      <th>V191</th>\n","      <th>V192</th>\n","      <th>V193</th>\n","      <th>V194</th>\n","      <th>V195</th>\n","      <th>V196</th>\n","      <th>V197</th>\n","      <th>V198</th>\n","      <th>V199</th>\n","      <th>V200</th>\n","      <th>V201</th>\n","      <th>V202</th>\n","      <th>V203</th>\n","      <th>V204</th>\n","      <th>V205</th>\n","      <th>V206</th>\n","      <th>V207</th>\n","      <th>V208</th>\n","      <th>V209</th>\n","      <th>V210</th>\n","      <th>V211</th>\n","      <th>V212</th>\n","      <th>V213</th>\n","      <th>V214</th>\n","      <th>V215</th>\n","      <th>V216</th>\n","      <th>V217</th>\n","      <th>V218</th>\n","      <th>V219</th>\n","      <th>V220</th>\n","      <th>V221</th>\n","      <th>V222</th>\n","      <th>V223</th>\n","      <th>V224</th>\n","      <th>V225</th>\n","      <th>V226</th>\n","      <th>V227</th>\n","      <th>V228</th>\n","      <th>V229</th>\n","      <th>V230</th>\n","      <th>V231</th>\n","      <th>V232</th>\n","      <th>V233</th>\n","      <th>V234</th>\n","      <th>V235</th>\n","      <th>V236</th>\n","      <th>V237</th>\n","      <th>V238</th>\n","      <th>V239</th>\n","      <th>V240</th>\n","      <th>V241</th>\n","      <th>V242</th>\n","      <th>V243</th>\n","      <th>V244</th>\n","      <th>V245</th>\n","      <th>V246</th>\n","      <th>V247</th>\n","      <th>V248</th>\n","      <th>V249</th>\n","      <th>V250</th>\n","      <th>V251</th>\n","      <th>V252</th>\n","      <th>V253</th>\n","      <th>V254</th>\n","      <th>V255</th>\n","      <th>V256</th>\n","      <th>V257</th>\n","      <th>V258</th>\n","      <th>V259</th>\n","      <th>V260</th>\n","      <th>V261</th>\n","      <th>V262</th>\n","      <th>V263</th>\n","      <th>V264</th>\n","      <th>V265</th>\n","      <th>V266</th>\n","      <th>V267</th>\n","      <th>V268</th>\n","      <th>V269</th>\n","      <th>V270</th>\n","      <th>V271</th>\n","      <th>V272</th>\n","      <th>V273</th>\n","      <th>V274</th>\n","      <th>V275</th>\n","      <th>V276</th>\n","      <th>V277</th>\n","      <th>V278</th>\n","      <th>V279</th>\n","      <th>V280</th>\n","      <th>V281</th>\n","      <th>V282</th>\n","      <th>V283</th>\n","      <th>V284</th>\n","      <th>V285</th>\n","      <th>V286</th>\n","      <th>V287</th>\n","      <th>V288</th>\n","      <th>V289</th>\n","      <th>V290</th>\n","      <th>V291</th>\n","      <th>V292</th>\n","      <th>V293</th>\n","      <th>V294</th>\n","      <th>V295</th>\n","      <th>V296</th>\n","      <th>V297</th>\n","      <th>V298</th>\n","      <th>V299</th>\n","      <th>V300</th>\n","      <th>V301</th>\n","      <th>V302</th>\n","      <th>V303</th>\n","      <th>V304</th>\n","      <th>V305</th>\n","      <th>V306</th>\n","      <th>V307</th>\n","      <th>V308</th>\n","      <th>V309</th>\n","      <th>V310</th>\n","      <th>V311</th>\n","      <th>V312</th>\n","      <th>V313</th>\n","      <th>V314</th>\n","      <th>V315</th>\n","      <th>V316</th>\n","      <th>V317</th>\n","      <th>V318</th>\n","      <th>V319</th>\n","      <th>V320</th>\n","      <th>V321</th>\n","      <th>id_01</th>\n","      <th>id_02</th>\n","      <th>id_05</th>\n","      <th>id_06</th>\n","      <th>id_11</th>\n","      <th>id_12</th>\n","      <th>id_13</th>\n","      <th>id_15</th>\n","      <th>id_16</th>\n","      <th>id_17</th>\n","      <th>id_19</th>\n","      <th>id_20</th>\n","      <th>id_28</th>\n","      <th>id_29</th>\n","      <th>id_31</th>\n","      <th>id_35</th>\n","      <th>id_36</th>\n","      <th>id_37</th>\n","      <th>id_38</th>\n","      <th>DeviceType</th>\n","      <th>_Weekdays</th>\n","      <th>_Hours</th>\n","      <th>_Days</th>\n","      <th>P_emaildomain_bin</th>\n","      <th>P_emaildomain_suffix</th>\n","      <th>R_emaildomain_bin</th>\n","      <th>R_emaildomain_suffix</th>\n","      <th>device_name</th>\n","      <th>had_id</th>\n","      <th>_Month</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>189.95</td>\n","      <td>W</td>\n","      <td>13534</td>\n","      <td>105.0</td>\n","      <td>150.0</td>\n","      <td>visa</td>\n","      <td>226.0</td>\n","      <td>debit</td>\n","      <td>220.0</td>\n","      <td>87.0</td>\n","      <td>13.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>41.0</td>\n","      <td>2.0</td>\n","      <td>F</td>\n","      <td>F</td>\n","      <td>F</td>\n","      <td>M2</td>\n","      <td>F</td>\n","      <td>F</td>\n","      <td>F</td>\n","      <td>F</td>\n","      <td>T</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>92.949997</td>\n","      <td>92.949997</td>\n","      <td>0.0</td>\n","      <td>92.949997</td>\n","      <td>92.949997</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>49.386378</td>\n","      <td>-601.155611</td>\n","      <td>449.607780</td>\n","      <td>-3.025672</td>\n","      <td>22.899776</td>\n","      <td>-46.529313</td>\n","      <td>-4.799213</td>\n","      <td>5.909761</td>\n","      <td>0.319198</td>\n","      <td>358.533228</td>\n","      <td>3050.723846</td>\n","      <td>-370.488113</td>\n","      <td>-72.222500</td>\n","      <td>-38.247117</td>\n","      <td>-20.134406</td>\n","      <td>0.0</td>\n","      <td>178.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>8.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>7.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>6.0</td>\n","      <td>2.0</td>\n","      <td>21.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>7.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>17.0</td>\n","      <td>-140.506602</td>\n","      <td>90.693080</td>\n","      <td>19.106982</td>\n","      <td>2.872524</td>\n","      <td>-94.728002</td>\n","      <td>-17.929145</td>\n","      <td>37.719204</td>\n","      <td>6.320434</td>\n","      <td>-3.944549</td>\n","      <td>1.071223</td>\n","      <td>-31.086690</td>\n","      <td>-68.453125</td>\n","      <td>-42.108457</td>\n","      <td>71.617984</td>\n","      <td>10.015859</td>\n","      <td>-14.607583</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>185.899994</td>\n","      <td>185.899994</td>\n","      <td>0.0</td>\n","      <td>92.949997</td>\n","      <td>0.0</td>\n","      <td>92.949997</td>\n","      <td>92.949997</td>\n","      <td>92.949997</td>\n","      <td>92.949997</td>\n","      <td>0.0</td>\n","      <td>92.949997</td>\n","      <td>92.949997</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>-99.0</td>\n","      <td>112576.0</td>\n","      <td>48.0</td>\n","      <td>-89.0</td>\n","      <td>100.197950</td>\n","      <td>Found</td>\n","      <td>42.0</td>\n","      <td>Found</td>\n","      <td>Found</td>\n","      <td>188.0</td>\n","      <td>653.0</td>\n","      <td>536.0</td>\n","      <td>New</td>\n","      <td>Found</td>\n","      <td>Chrome</td>\n","      <td>F</td>\n","      <td>F</td>\n","      <td>F</td>\n","      <td>T</td>\n","      <td>desktop</td>\n","      <td>3</td>\n","      <td>16</td>\n","      <td>15</td>\n","      <td>yahoo</td>\n","      <td>com</td>\n","      <td>apple</td>\n","      <td>com</td>\n","      <td>STELLAR</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>27.95</td>\n","      <td>W</td>\n","      <td>4663</td>\n","      <td>490.0</td>\n","      <td>150.0</td>\n","      <td>visa</td>\n","      <td>166.0</td>\n","      <td>debit</td>\n","      <td>251.0</td>\n","      <td>87.0</td>\n","      <td>70.0</td>\n","      <td>10.0</td>\n","      <td>20.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>11.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>12.0</td>\n","      <td>0.0</td>\n","      <td>12.0</td>\n","      <td>0.0</td>\n","      <td>53.0</td>\n","      <td>10.0</td>\n","      <td>6.0</td>\n","      <td>6.0</td>\n","      <td>6.0</td>\n","      <td>0.0</td>\n","      <td>55.0</td>\n","      <td>6.0</td>\n","      <td>6.0</td>\n","      <td>6.0</td>\n","      <td>T</td>\n","      <td>T</td>\n","      <td>F</td>\n","      <td>M2</td>\n","      <td>F</td>\n","      <td>F</td>\n","      <td>F</td>\n","      <td>F</td>\n","      <td>F</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>141.949997</td>\n","      <td>141.949997</td>\n","      <td>0.0</td>\n","      <td>141.949997</td>\n","      <td>141.949997</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>961.145632</td>\n","      <td>-619.745193</td>\n","      <td>37.610188</td>\n","      <td>-6.907019</td>\n","      <td>-9.409274</td>\n","      <td>-148.582189</td>\n","      <td>2.781550</td>\n","      <td>-7.255751</td>\n","      <td>108.906406</td>\n","      <td>-268.309453</td>\n","      <td>667.192683</td>\n","      <td>-271.741063</td>\n","      <td>165.456314</td>\n","      <td>15.012737</td>\n","      <td>-104.631380</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>6.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>6.0</td>\n","      <td>2.0</td>\n","      <td>21.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>208.319400</td>\n","      <td>337.419192</td>\n","      <td>96.198823</td>\n","      <td>12.102134</td>\n","      <td>51.892323</td>\n","      <td>11.733198</td>\n","      <td>-3.563503</td>\n","      <td>-1.667548</td>\n","      <td>1.787430</td>\n","      <td>-3.095251</td>\n","      <td>42.278810</td>\n","      <td>119.593034</td>\n","      <td>-88.127266</td>\n","      <td>28.233834</td>\n","      <td>49.703917</td>\n","      <td>-9.337568</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>141.949997</td>\n","      <td>141.949997</td>\n","      <td>0.0</td>\n","      <td>141.949997</td>\n","      <td>0.0</td>\n","      <td>141.949997</td>\n","      <td>141.949997</td>\n","      <td>141.949997</td>\n","      <td>141.949997</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>-56.0</td>\n","      <td>230945.0</td>\n","      <td>16.0</td>\n","      <td>-89.0</td>\n","      <td>100.000175</td>\n","      <td>Found</td>\n","      <td>42.0</td>\n","      <td>Found</td>\n","      <td>Found</td>\n","      <td>188.0</td>\n","      <td>653.0</td>\n","      <td>536.0</td>\n","      <td>New</td>\n","      <td>Found</td>\n","      <td>Chrome</td>\n","      <td>F</td>\n","      <td>F</td>\n","      <td>F</td>\n","      <td>T</td>\n","      <td>desktop</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>google</td>\n","      <td>com</td>\n","      <td>apple</td>\n","      <td>com</td>\n","      <td>STELLAR</td>\n","      <td>1</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>407.50</td>\n","      <td>W</td>\n","      <td>8701</td>\n","      <td>111.0</td>\n","      <td>150.0</td>\n","      <td>visa</td>\n","      <td>226.0</td>\n","      <td>debit</td>\n","      <td>310.0</td>\n","      <td>87.0</td>\n","      <td>10.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>543.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>T</td>\n","      <td>T</td>\n","      <td>T</td>\n","      <td>M1</td>\n","      <td>F</td>\n","      <td>F</td>\n","      <td>F</td>\n","      <td>T</td>\n","      <td>T</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>961.5</td>\n","      <td>961.500000</td>\n","      <td>961.500000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>961.5</td>\n","      <td>961.5</td>\n","      <td>961.5</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>6.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>823.546748</td>\n","      <td>-80.629886</td>\n","      <td>-764.133999</td>\n","      <td>33.808809</td>\n","      <td>-1.000731</td>\n","      <td>-0.342470</td>\n","      <td>-2.075324</td>\n","      <td>-5.505205</td>\n","      <td>7.168003</td>\n","      <td>594.610971</td>\n","      <td>191.138848</td>\n","      <td>10.317362</td>\n","      <td>-81.101185</td>\n","      <td>77.299374</td>\n","      <td>-61.736930</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>11.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>6.0</td>\n","      <td>2.0</td>\n","      <td>21.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.0</td>\n","      <td>2.0</td>\n","      <td>54.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>-19.148046</td>\n","      <td>343.764064</td>\n","      <td>245.512514</td>\n","      <td>4.536676</td>\n","      <td>102.791669</td>\n","      <td>-4.429336</td>\n","      <td>-5.236441</td>\n","      <td>1.108247</td>\n","      <td>3.764343</td>\n","      <td>-0.769841</td>\n","      <td>49.230310</td>\n","      <td>-147.721740</td>\n","      <td>175.518377</td>\n","      <td>62.349444</td>\n","      <td>-93.306142</td>\n","      <td>-44.550055</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>5.0</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1776.5</td>\n","      <td>1776.500000</td>\n","      <td>1776.500000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1369.0</td>\n","      <td>1369.000000</td>\n","      <td>1369.000000</td>\n","      <td>407.5</td>\n","      <td>407.5</td>\n","      <td>407.5</td>\n","      <td>-93.0</td>\n","      <td>86225.0</td>\n","      <td>26.0</td>\n","      <td>-61.0</td>\n","      <td>99.866097</td>\n","      <td>Found</td>\n","      <td>42.0</td>\n","      <td>Found</td>\n","      <td>Found</td>\n","      <td>188.0</td>\n","      <td>653.0</td>\n","      <td>536.0</td>\n","      <td>New</td>\n","      <td>Found</td>\n","      <td>Chrome</td>\n","      <td>F</td>\n","      <td>F</td>\n","      <td>F</td>\n","      <td>T</td>\n","      <td>desktop</td>\n","      <td>4</td>\n","      <td>20</td>\n","      <td>15</td>\n","      <td>aol</td>\n","      <td>com</td>\n","      <td>apple</td>\n","      <td>com</td>\n","      <td>STELLAR</td>\n","      <td>1</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>29.00</td>\n","      <td>W</td>\n","      <td>7069</td>\n","      <td>111.0</td>\n","      <td>150.0</td>\n","      <td>mastercard</td>\n","      <td>202.0</td>\n","      <td>debit</td>\n","      <td>325.0</td>\n","      <td>87.0</td>\n","      <td>7.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>201.0</td>\n","      <td>30.0</td>\n","      <td>0.0</td>\n","      <td>207.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>T</td>\n","      <td>T</td>\n","      <td>T</td>\n","      <td>M1</td>\n","      <td>F</td>\n","      <td>F</td>\n","      <td>F</td>\n","      <td>F</td>\n","      <td>T</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>61.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>-16.491188</td>\n","      <td>-640.625897</td>\n","      <td>-352.501348</td>\n","      <td>2.780901</td>\n","      <td>-12.216878</td>\n","      <td>39.382364</td>\n","      <td>-1.665714</td>\n","      <td>27.422245</td>\n","      <td>4.419788</td>\n","      <td>794.046246</td>\n","      <td>414.318008</td>\n","      <td>-395.839060</td>\n","      <td>27.819971</td>\n","      <td>16.832865</td>\n","      <td>-61.759354</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>6.0</td>\n","      <td>2.0</td>\n","      <td>21.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>7.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>286.831530</td>\n","      <td>-15.282319</td>\n","      <td>-1.372384</td>\n","      <td>-8.497692</td>\n","      <td>-22.647806</td>\n","      <td>-8.044510</td>\n","      <td>1.801023</td>\n","      <td>3.982033</td>\n","      <td>-17.213138</td>\n","      <td>0.260067</td>\n","      <td>14.046873</td>\n","      <td>103.279462</td>\n","      <td>-73.452810</td>\n","      <td>22.467517</td>\n","      <td>18.830369</td>\n","      <td>-97.694330</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>-56.0</td>\n","      <td>1298.0</td>\n","      <td>26.0</td>\n","      <td>-89.0</td>\n","      <td>100.139476</td>\n","      <td>Found</td>\n","      <td>42.0</td>\n","      <td>Found</td>\n","      <td>Found</td>\n","      <td>188.0</td>\n","      <td>653.0</td>\n","      <td>536.0</td>\n","      <td>New</td>\n","      <td>Found</td>\n","      <td>Chrome</td>\n","      <td>F</td>\n","      <td>F</td>\n","      <td>F</td>\n","      <td>T</td>\n","      <td>desktop</td>\n","      <td>3</td>\n","      <td>23</td>\n","      <td>8</td>\n","      <td>google</td>\n","      <td>com</td>\n","      <td>apple</td>\n","      <td>com</td>\n","      <td>STELLAR</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>54.50</td>\n","      <td>W</td>\n","      <td>7919</td>\n","      <td>194.0</td>\n","      <td>150.0</td>\n","      <td>mastercard</td>\n","      <td>202.0</td>\n","      <td>debit</td>\n","      <td>181.0</td>\n","      <td>87.0</td>\n","      <td>22.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>263.0</td>\n","      <td>263.0</td>\n","      <td>242.0</td>\n","      <td>0.0</td>\n","      <td>40.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>T</td>\n","      <td>T</td>\n","      <td>T</td>\n","      <td>M0</td>\n","      <td>T</td>\n","      <td>F</td>\n","      <td>F</td>\n","      <td>F</td>\n","      <td>T</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>46.458250</td>\n","      <td>533.443166</td>\n","      <td>121.223080</td>\n","      <td>1.736350</td>\n","      <td>-22.621447</td>\n","      <td>-43.929972</td>\n","      <td>2.710370</td>\n","      <td>-16.442391</td>\n","      <td>-11.334086</td>\n","      <td>187.263072</td>\n","      <td>-51.003672</td>\n","      <td>205.247551</td>\n","      <td>89.382867</td>\n","      <td>42.032791</td>\n","      <td>63.956224</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>6.0</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>6.0</td>\n","      <td>35.0</td>\n","      <td>21.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>74.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>-20.745677</td>\n","      <td>-652.297525</td>\n","      <td>54.223172</td>\n","      <td>-14.311653</td>\n","      <td>-17.943575</td>\n","      <td>-17.402072</td>\n","      <td>-31.061075</td>\n","      <td>2.060123</td>\n","      <td>24.077774</td>\n","      <td>-7.284072</td>\n","      <td>-41.579208</td>\n","      <td>-120.195796</td>\n","      <td>-100.749158</td>\n","      <td>260.444797</td>\n","      <td>-107.966703</td>\n","      <td>-29.520311</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>-47.0</td>\n","      <td>126421.0</td>\n","      <td>26.0</td>\n","      <td>-89.0</td>\n","      <td>95.205613</td>\n","      <td>Found</td>\n","      <td>42.0</td>\n","      <td>Found</td>\n","      <td>Found</td>\n","      <td>188.0</td>\n","      <td>653.0</td>\n","      <td>536.0</td>\n","      <td>New</td>\n","      <td>Found</td>\n","      <td>Chrome</td>\n","      <td>F</td>\n","      <td>F</td>\n","      <td>F</td>\n","      <td>T</td>\n","      <td>desktop</td>\n","      <td>6</td>\n","      <td>21</td>\n","      <td>31</td>\n","      <td>google</td>\n","      <td>com</td>\n","      <td>apple</td>\n","      <td>com</td>\n","      <td>STELLAR</td>\n","      <td>1</td>\n","      <td>12</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   isFraud  TransactionAmt ProductCD  card1  card2  card3       card4  card5  card6  addr1  addr2  dist1    C1    C2   C3   C4    C5    C6   C7   C8    C9  C10   C11  C12   C13   C14     D1     D2     D3   D4     D5  D10   D11  D15 M1 M2 M3  M4 M5 M6 M7 M8 M9   V1   V2   V3   V4   V5   V6   V7   V8   V9  V10  V11  V12  V13  V14  V15  V16  V17  V18  V19  V20  V21  V22  V23  V24  V25  V26  V27  V28  V29  V30  V31  V32  V33  V34  V35  V36  V37  V38  V39  V40  V41  V42  V43  V44  V45  V46  V47  V48  V49  V50  V51  V52  V53  V54  V55  V56  V57  V58  V59  V60  V61  V62  V63  V64  V65  V66  V67  V68  V69  V70  V71  V72  V73  V74  V75  V76  V77  V78  V79  V80  V81  V82  V83  V84  V85  V86  V87  V88  V89  V90  V91  V92  V93  V94  V95  V96  V97  V98  V99  V100  V101  V102  V103  V104  V105  V106  V107  V108  V109  V110  V111  V112  V113  V114  V115  V116  V117  V118  V119  V120  V121  V122  V123  V124  V125   V126        V127        V128  V129        V130        V131   V132   V133   V134  \\\n","0        0          189.95         W  13534  105.0  150.0        visa  226.0  debit  220.0   87.0   13.0   1.0   2.0  0.0  0.0   2.0   1.0  0.0  0.0   1.0  0.0   1.0  0.0   2.0   1.0    2.0    2.0    2.0  2.0    2.0  2.0  41.0  2.0  F  F  F  M2  F  F  F  F  T  0.0  2.0  2.0  2.0  0.0  2.0  2.0  2.0  2.0  1.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  1.0  2.0  0.0  0.0  0.0  0.0  1.0  1.0  2.0  2.0  0.0  0.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0  1.0  2.0  0.0  0.0  0.0  1.0  1.0  2.0  2.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  0.0  1.0  2.0  0.0  0.0  0.0  0.0  1.0  1.0  2.0  2.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  0.0  1.0  2.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  1.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0    0.0   92.949997   92.949997   0.0   92.949997   92.949997    0.0    0.0    0.0   \n","1        0           27.95         W   4663  490.0  150.0        visa  166.0  debit  251.0   87.0   70.0  10.0  20.0  0.0  0.0  11.0  11.0  0.0  0.0  12.0  0.0  12.0  0.0  53.0  10.0    6.0    6.0    6.0  0.0   55.0  6.0   6.0  6.0  T  T  F  M2  F  F  F  F  F  1.0  1.0  1.0  1.0  2.0  1.0  1.0  1.0  1.0  1.0  2.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  1.0  2.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  1.0  2.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  1.0  2.0  0.0  0.0  1.0  1.0  1.0  0.0  1.0  2.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  1.0  2.0  0.0  0.0  1.0  1.0  1.0  0.0  1.0  2.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  1.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0    0.0  141.949997  141.949997   0.0  141.949997  141.949997    0.0    0.0    0.0   \n","2        0          407.50         W   8701  111.0  150.0        visa  226.0  debit  310.0   87.0   10.0   1.0   2.0  0.0  0.0   0.0   1.0  0.0  0.0   1.0  0.0   1.0  0.0   0.0   0.0    1.0  543.0    1.0  1.0    1.0  1.0   1.0  1.0  T  T  T  M1  F  F  F  T  T  1.0  2.0  2.0  1.0  1.0  1.0  1.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  2.0  2.0  0.0  0.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  2.0  2.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  2.0  2.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  2.0  2.0  0.0  0.0   0.0   2.0   2.0   2.0   0.0   0.0   0.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0  961.5  961.500000  961.500000   0.0    0.000000    0.000000  961.5  961.5  961.5   \n","3        0           29.00         W   7069  111.0  150.0  mastercard  202.0  debit  325.0   87.0    7.0   1.0   1.0  0.0  0.0   0.0   1.0  0.0  0.0   1.0  0.0   1.0  0.0   1.0   1.0    0.0  201.0   30.0  0.0  207.0  0.0   0.0  0.0  T  T  T  M1  F  F  F  F  T  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0    0.0    0.000000    0.000000   0.0    0.000000    0.000000    0.0    0.0    0.0   \n","4        0           54.50         W   7919  194.0  150.0  mastercard  202.0  debit  181.0   87.0   22.0   1.0   3.0  0.0  0.0   0.0   2.0  0.0  0.0   2.0  0.0   2.0  0.0   2.0   1.0  263.0  263.0  242.0  0.0   40.0  0.0   0.0  0.0  T  T  T  M0  T  F  F  F  T  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0    0.0    0.000000    0.000000   0.0    0.000000    0.000000    0.0    0.0    0.0   \n","\n","   V135  V136  V137  V167  V168  V169  V170  V171  V172  V173  V174  V175  V176  V177  V178  V179  V180  V181  V182  V183  V184  V185  V186  V187  V188  V189  V190  V191  V192  V193  V194  V195  V196  V197  V198  V199  V200  V201        V202        V203        V204       V205       V206        V207      V208       V209        V210        V211         V212        V213        V214       V215        V216  V217   V218  V219  V220  V221  V222  V223  V224  V225  V226  V227  V228  V229  V230  V231  V232  V233  V234  V235  V236  V237  V238  V239  V240  V241  V242  V243  V244  V245  V246  V247  V248  V249  V250  V251  V252  V253  V254  V255  V256  V257  V258  V259  V260  V261  V262        V263        V264        V265       V266        V267       V268       V269      V270       V271      V272       V273        V274        V275        V276        V277       V278  V279  V280  V281  V282  V283  V284  V285  V286  V287  V288  V289  V290  V291  V292  V293  V294  V295  V296  V297  V298  V299  V300  \\\n","0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   5.0   0.0   0.0   0.0   0.0   4.0   0.0   0.0   0.0   0.0   0.0   0.0   2.0   0.0   1.0   2.0   2.0   2.0   1.0   2.0   4.0   4.0   4.0   0.0   1.0   4.0   0.0   1.0   2.0   2.0   2.0   49.386378 -601.155611  449.607780  -3.025672  22.899776  -46.529313 -4.799213   5.909761    0.319198  358.533228  3050.723846 -370.488113  -72.222500 -38.247117  -20.134406   0.0  178.0   0.0   0.0   8.0   1.0   0.0   0.0   0.0   0.0   0.0   4.0   4.0   4.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   2.0   2.0   2.0   2.0   7.0   2.0   2.0   6.0   2.0  21.0   1.0   1.0   7.0   2.0   2.0   0.0   0.0   3.0   2.0   1.0   0.0   0.0  17.0 -140.506602   90.693080   19.106982   2.872524  -94.728002 -17.929145  37.719204  6.320434  -3.944549  1.071223 -31.086690  -68.453125  -42.108457   71.617984   10.015859 -14.607583   0.0   2.0   0.0   2.0   3.0   0.0   1.0   0.0   1.0   1.0   1.0   1.0   2.0   2.0   0.0   1.0   1.0   0.0   0.0   0.0   0.0   0.0   \n","1   0.0   0.0   0.0   0.0   0.0   0.0   4.0   4.0   0.0   0.0   0.0   0.0   5.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   3.0   2.0   2.0   2.0   2.0   2.0   4.0   4.0   4.0   1.0   1.0   4.0   0.0   0.0   2.0   2.0   2.0  961.145632 -619.745193   37.610188  -6.907019  -9.409274 -148.582189  2.781550  -7.255751  108.906406 -268.309453   667.192683 -271.741063  165.456314  15.012737 -104.631380   3.0    0.0   0.0   0.0   4.0   4.0   0.0   0.0   0.0   0.0   0.0   6.0   4.0   4.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   2.0   2.0   2.0   2.0   2.0   1.0   2.0   6.0   2.0  21.0   0.0   0.0   7.0   2.0   2.0   0.0   0.0   2.0   2.0   2.0   0.0   4.0   2.0  208.319400  337.419192   96.198823  12.102134   51.892323  11.733198  -3.563503 -1.667548   1.787430 -3.095251  42.278810  119.593034  -88.127266   28.233834   49.703917  -9.337568   0.0   1.0   0.0   1.0   2.0   0.0   1.0   0.0   1.0   1.0   1.0   1.0   1.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n","2   0.0   0.0   0.0   0.0   0.0   0.0   4.0   6.0   0.0   0.0   0.0   1.0   4.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   2.0   2.0   1.0   1.0   2.0   4.0   4.0   4.0   0.0   0.0   4.0   0.0   0.0   2.0   2.0   2.0  823.546748  -80.629886 -764.133999  33.808809  -1.000731   -0.342470 -2.075324  -5.505205    7.168003  594.610971   191.138848   10.317362  -81.101185  77.299374  -61.736930   0.0    0.0   0.0   0.0   4.0   4.0   0.0   0.0   0.0   0.0   0.0   4.0   4.0   4.0  11.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0   1.0   2.0   2.0   2.0   2.0   2.0   0.0   2.0   6.0   2.0  21.0   0.0   0.0   7.0   2.0  54.0   0.0   0.0   2.0   2.0   2.0   0.0   0.0   0.0  -19.148046  343.764064  245.512514   4.536676  102.791669  -4.429336  -5.236441  1.108247   3.764343 -0.769841  49.230310 -147.721740  175.518377   62.349444  -93.306142 -44.550055   4.0   4.0   0.0   5.0   5.0   0.0   0.0   0.0   0.0   0.0   0.0   2.0   2.0   2.0   3.0   3.0   3.0   1.0   1.0   1.0   1.0   1.0   \n","3   0.0   0.0   0.0   1.0   4.0   0.0   4.0   3.0   0.0   0.0   0.0   0.0   4.0   0.0   0.0   1.0   1.0   0.0  61.0   0.0   0.0   0.0   2.0   2.0   2.0   0.0   4.0   4.0   4.0   4.0   0.0   0.0   4.0   0.0   0.0   2.0   2.0   2.0  -16.491188 -640.625897 -352.501348   2.780901 -12.216878   39.382364 -1.665714  27.422245    4.419788  794.046246   414.318008 -395.839060   27.819971  16.832865  -61.759354   0.0    0.0   4.0   0.0   4.0   4.0   0.0   0.0   0.0   0.0   0.0   4.0   4.0   4.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   2.0   2.0   2.0   2.0   2.0   2.0   2.0   6.0   2.0  21.0   0.0   1.0   7.0   2.0   2.0   2.0   0.0   2.0   2.0   2.0   0.0   0.0   0.0  286.831530  -15.282319   -1.372384  -8.497692  -22.647806  -8.044510   1.801023  3.982033 -17.213138  0.260067  14.046873  103.279462  -73.452810   22.467517   18.830369 -97.694330   0.0   0.0   0.0   1.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n","4   0.0   0.0   0.0   0.0   4.0   0.0   4.0   3.0   0.0   0.0   0.0   0.0   4.0   0.0   0.0   0.0   0.0   0.0   2.0   0.0   0.0   0.0   2.0   2.0   2.0   2.0   2.0   4.0   4.0   4.0   0.0   0.0   4.0   1.0   0.0   2.0   2.0   1.0   46.458250  533.443166  121.223080   1.736350 -22.621447  -43.929972  2.710370 -16.442391  -11.334086  187.263072   -51.003672  205.247551   89.382867  42.032791   63.956224   0.0    0.0   3.0   0.0   4.0   5.0   0.0   0.0   0.0   2.0   0.0   6.0   4.0   4.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   2.0   2.0   2.0   2.0   2.0   2.0   2.0   6.0  35.0  21.0   0.0   0.0   7.0   2.0   2.0   1.0  74.0   2.0   2.0   1.0   0.0   0.0   0.0  -20.745677 -652.297525   54.223172 -14.311653  -17.943575 -17.402072 -31.061075  2.060123  24.077774 -7.284072 -41.579208 -120.195796 -100.749158  260.444797 -107.966703 -29.520311   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n","\n","   V301  V302  V303  V304  V305    V306         V307         V308  V309        V310  V311        V312        V313        V314        V315    V316         V317         V318   V319   V320   V321  id_01     id_02  id_05  id_06       id_11  id_12  id_13  id_15  id_16  id_17  id_19  id_20 id_28  id_29   id_31 id_35 id_36 id_37 id_38 DeviceType  _Weekdays  _Hours  _Days P_emaildomain_bin P_emaildomain_suffix R_emaildomain_bin R_emaildomain_suffix device_name  had_id  _Month  \n","0   0.0   0.0   0.0   0.0   1.0     0.0   185.899994   185.899994   0.0   92.949997   0.0   92.949997   92.949997   92.949997   92.949997     0.0    92.949997    92.949997    0.0    0.0    0.0  -99.0  112576.0   48.0  -89.0  100.197950  Found   42.0  Found  Found  188.0  653.0  536.0   New  Found  Chrome     F     F     F     T    desktop          3      16     15             yahoo                  com             apple                  com     STELLAR       1       2  \n","1   0.0   0.0   0.0   0.0   1.0     0.0   141.949997   141.949997   0.0  141.949997   0.0  141.949997  141.949997  141.949997  141.949997     0.0     0.000000     0.000000    0.0    0.0    0.0  -56.0  230945.0   16.0  -89.0  100.000175  Found   42.0  Found  Found  188.0  653.0  536.0   New  Found  Chrome     F     F     F     T    desktop          2       0      2            google                  com             apple                  com     STELLAR       1       5  \n","2   1.0   0.0   0.0   0.0   1.0  1776.5  1776.500000  1776.500000   0.0    0.000000   0.0    0.000000    0.000000    0.000000    0.000000  1369.0  1369.000000  1369.000000  407.5  407.5  407.5  -93.0   86225.0   26.0  -61.0   99.866097  Found   42.0  Found  Found  188.0  653.0  536.0   New  Found  Chrome     F     F     F     T    desktop          4      20     15               aol                  com             apple                  com     STELLAR       1      12  \n","3   0.0   0.0   0.0   0.0   1.0     0.0     0.000000     0.000000   0.0    0.000000   0.0    0.000000    0.000000    0.000000    0.000000     0.0     0.000000     0.000000    0.0    0.0    0.0  -56.0    1298.0   26.0  -89.0  100.139476  Found   42.0  Found  Found  188.0  653.0  536.0   New  Found  Chrome     F     F     F     T    desktop          3      23      8            google                  com             apple                  com     STELLAR       1       2  \n","4   0.0   0.0   0.0   0.0   1.0     0.0     0.000000     0.000000   0.0    0.000000   0.0    0.000000    0.000000    0.000000    0.000000     0.0     0.000000     0.000000    0.0    0.0    0.0  -47.0  126421.0   26.0  -89.0   95.205613  Found   42.0  Found  Found  188.0  653.0  536.0   New  Found  Chrome     F     F     F     T    desktop          6      21     31            google                  com             apple                  com     STELLAR       1      12  "]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"nH_AZ9NEnJ-I","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"dfe074d4-0299-463b-c4b4-ab11451476e0","executionInfo":{"status":"ok","timestamp":1577613512058,"user_tz":-120,"elapsed":9043,"user":{"displayName":"din bav","photoUrl":"","userId":"06942881549123238413"}}},"source":["d.select_dtypes(include=['object']).columns"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['ProductCD', 'card4', 'card6', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'id_12', 'id_15', 'id_16', 'id_28', 'id_29', 'id_31', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'P_emaildomain_bin', 'P_emaildomain_suffix', 'R_emaildomain_bin', 'R_emaildomain_suffix', 'device_name'], dtype='object')"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"4SHCQmeZnQht","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"8997cac8-b2e1-43f4-e03a-7e3c41a6fa7a","executionInfo":{"status":"ok","timestamp":1577613512432,"user_tz":-120,"elapsed":8641,"user":{"displayName":"din bav","photoUrl":"","userId":"06942881549123238413"}}},"source":["len(d.select_dtypes(include=['object']).columns)"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["28"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"OWmmQbrMjssa","colab_type":"code","colab":{}},"source":["cat_cols=['ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n","       'addr1', 'addr2', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9',\n","       'id_12', 'id_13', 'id_15', 'id_16', 'id_17', 'id_19', 'id_20', 'id_28',\n","       'id_29', 'id_31', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType',\n","       '_Weekdays', '_Hours', '_Days', 'P_emaildomain_bin',\n","       'P_emaildomain_suffix', 'R_emaildomain_bin', 'R_emaildomain_suffix',\n","       'device_name', 'had_id', '_Month']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QdF6Ha_zm-ZN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"2fb3a4d6-33d9-4698-c391-d117479e5527","executionInfo":{"status":"ok","timestamp":1577613512434,"user_tz":-120,"elapsed":6337,"user":{"displayName":"din bav","photoUrl":"","userId":"06942881549123238413"}}},"source":["len(cat_cols)"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["43"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"IMxH4Gw9kNFt","colab_type":"code","colab":{}},"source":["for i in range(len(cat_cols)):\n","  col=cat_cols[i]\n","  d[col]=d[col].astype(str) \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gG-w4GdNmxPL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":285},"outputId":"5b05a898-5286-46b1-e1e8-3b3fc014b5ac","executionInfo":{"status":"ok","timestamp":1577609000725,"user_tz":-120,"elapsed":9516,"user":{"displayName":"din bav","photoUrl":"","userId":"06942881549123238413"}}},"source":["print(d.select_dtypes(include=['object']))"],"execution_count":71,"outputs":[{"output_type":"stream","text":["       ProductCD  card1  card2  card3       card4  card5   card6  addr1 addr2 M1 M2 M3  M4 M5 M6 M7 M8 M9     id_12 id_13  id_15  id_16  id_17  id_19  id_20  id_28  id_29   id_31 id_35 id_36 id_37 id_38 DeviceType _Weekdays _Hours _Days P_emaildomain_bin P_emaildomain_suffix R_emaildomain_bin R_emaildomain_suffix device_name had_id _Month\n","0              W  13534  105.0  150.0        visa  226.0   debit  220.0  87.0  F  F  F  M2  F  F  F  F  T     Found  42.0  Found  Found  188.0  653.0  536.0    New  Found  Chrome     F     F     F     T    desktop         3     16    15             yahoo                  com             apple                  com     STELLAR      1      2\n","1              W   4663  490.0  150.0        visa  166.0   debit  251.0  87.0  T  T  F  M2  F  F  F  F  F     Found  42.0  Found  Found  188.0  653.0  536.0    New  Found  Chrome     F     F     F     T    desktop         2      0     2            google                  com             apple                  com     STELLAR      1      5\n","2              W   8701  111.0  150.0        visa  226.0   debit  310.0  87.0  T  T  T  M1  F  F  F  T  T     Found  42.0  Found  Found  188.0  653.0  536.0    New  Found  Chrome     F     F     F     T    desktop         4     20    15               aol                  com             apple                  com     STELLAR      1     12\n","3              W   7069  111.0  150.0  mastercard  202.0   debit  325.0  87.0  T  T  T  M1  F  F  F  F  T     Found  42.0  Found  Found  188.0  653.0  536.0    New  Found  Chrome     F     F     F     T    desktop         3     23     8            google                  com             apple                  com     STELLAR      1      2\n","4              W   7919  194.0  150.0  mastercard  202.0   debit  181.0  87.0  T  T  T  M0  T  F  F  F  T     Found  42.0  Found  Found  188.0  653.0  536.0    New  Found  Chrome     F     F     F     T    desktop         6     21    31            google                  com             apple                  com     STELLAR      1     12\n","...          ...    ...    ...    ...         ...    ...     ...    ...   ... .. .. ..  .. .. .. .. .. ..       ...   ...    ...    ...    ...    ...    ...    ...    ...     ...   ...   ...   ...   ...        ...       ...    ...   ...               ...                  ...               ...                  ...         ...    ...    ...\n","177158         W   2616  327.0  150.0    discover  102.0  credit  204.0  87.0  T  F  F  M0  T  F  F  F  T     Found  42.0  Found  Found  188.0  653.0  536.0    New  Found  Chrome     F     F     F     T    desktop         1     16    20            google                  com             apple                  com     STELLAR      1      2\n","177159         C  15885  545.0  185.0        visa  138.0   debit  141.0  18.0  F  F  F  M2  F  T  F  F  T  NotFound  64.0  Found  Found  225.0  266.0  325.0  Found  Found  Chrome     F     F     T     F     mobile         2      1    16            google                  com            google                  com      Huawei      1      5\n","177160         C   5943  296.0  185.0        visa  195.0   debit  141.0  18.0  F  F  F  M2  F  T  F  F  T  NotFound  52.0  Found  Found  225.0  254.0  507.0  Found  Found  Chrome     F     F     T     F    desktop         5     17     3         microsoft                  com         microsoft                  com     STELLAR      1      2\n","177161         C   8457  118.0  185.0        visa  226.0   debit  141.0  18.0  F  F  F  M2  F  T  F  F  T  NotFound  52.0  Found  Found  225.0  290.0  305.0  Found  Found  Chrome     F     F     T     F    desktop         4      2     6            google                  com            google                  com     STELLAR      1      4\n","177162         W   2616  271.0  150.0    discover  102.0  credit  177.0  87.0  T  T  F  M2  F  F  F  F  F     Found  42.0  Found  Found  188.0  653.0  536.0    New  Found  Chrome     F     F     F     T    desktop         6     20    10               aol                  com             apple                  com     STELLAR      1     12\n","\n","[177163 rows x 43 columns]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R7TDFct7pIaj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"31bd2f16-f47d-4e4f-817c-d4c90f9ba637","executionInfo":{"status":"ok","timestamp":1577613516583,"user_tz":-120,"elapsed":5420,"user":{"displayName":"din bav","photoUrl":"","userId":"06942881549123238413"}}},"source":["len(d.select_dtypes(include=['object']).columns)"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["43"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"evpahzXP3Zd5","colab_type":"code","colab":{}},"source":["data=d"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vRjeoRRJDPzE","colab_type":"code","colab":{}},"source":["#To change file name uploaded\n","df_MI=pd.read_csv('gdrive/My Drive/Colab Notebooks/Predictive Modeling/Fraud/DATA/3. fraud_remove_nulls_fill_random_distribution/features_MI_train-remove_nulls_fill_random_distribution.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-MDE3oREdMd3","colab_type":"code","colab":{}},"source":["features_25=data[list(df_MI['Column'].head(25))]\n","features_50=data[list(df_MI['Column'].head(50))]\n","features_75=data[list(df_MI['Column'].head(75))]\n","features_100=data[list(df_MI['Column'].head(100))]\n","features_150=data[list(df_MI['Column'].head(150))]\n","features_200=data[list(df_MI['Column'].head(200))]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VICedAg3UQQe","colab_type":"code","outputId":"efbb95e1-0960-4f8b-ad88-fda29a725e1f","executionInfo":{"status":"ok","timestamp":1577613516843,"user_tz":-120,"elapsed":1850,"user":{"displayName":"din bav","photoUrl":"","userId":"06942881549123238413"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["features_25.shape"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(177163, 25)"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"xeOiYQcYbPVC","colab_type":"text"},"source":["Spliting the data into features and target for isFraud=0 and isFraud=1"]},{"cell_type":"code","metadata":{"id":"Hzn2NI2HJ_kM","colab_type":"code","outputId":"eede35d3-c709-492a-fbd7-a7cb4ed2bdbb","executionInfo":{"status":"ok","timestamp":1577603933914,"user_tz":-120,"elapsed":1653,"user":{"displayName":"din bav","photoUrl":"","userId":"06942881549123238413"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["170964/6199"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["27.579286981771254"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"2s32wrjxWcQO","colab_type":"code","outputId":"fbc47012-3d7a-4c87-89aa-0bcaf09b6f57","executionInfo":{"status":"ok","timestamp":1577613593101,"user_tz":-120,"elapsed":1461,"user":{"displayName":"din bav","photoUrl":"","userId":"06942881549123238413"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["data_isFraud_1_indexes=list(data[data['isFraud']==1].index)\n","data_isFraud_0_indexes=list(data[data['isFraud']==0].index)\n","data_isFraud_1=data.iloc[data_isFraud_1_indexes , : ]\n","data_isFraud_0=data.iloc[data_isFraud_0_indexes , : ]\n","\n","target_isFraud_1=data_isFraud_1['isFraud'].head(1000)\n","print('target_isFraurd_1 shape: ',target_isFraud_1.shape)\n","\n","features_isFraud_1=data_isFraud_1.drop(['isFraud'],axis=1).head(1000)\n","print('features_isFraud_1 shape: ',features_isFraud_1.shape)\n","\n","target_isFraud_0=data_isFraud_0['isFraud'].head(27500)\n","print('target_isFraurd_0 shape: ',target_isFraud_0.shape)\n","\n","features_isFraud_0=data_isFraud_0.drop(['isFraud'],axis=1).head(27500)\n","print('features_isFraud_0 shape: ',features_isFraud_0.shape)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["target_isFraurd_1 shape:  (1000,)\n","features_isFraud_1 shape:  (1000, 364)\n","target_isFraurd_0 shape:  (27500,)\n","features_isFraud_0 shape:  (27500, 364)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cf3M76XBDPzI","colab_type":"code","colab":{}},"source":["features_25_0=features_isFraud_0[list(df_MI['Column'].head(25))]\n","features_50_0=features_isFraud_0[list(df_MI['Column'].head(50))]\n","features_75_0=features_isFraud_0[list(df_MI['Column'].head(75))]\n","features_100_0=features_isFraud_0[list(df_MI['Column'].head(100))]\n","features_150_0=features_isFraud_0[list(df_MI['Column'].head(150))]\n","features_200_0=features_isFraud_0[list(df_MI['Column'].head(200))]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dSiLKdMpdUXu","colab_type":"code","outputId":"10e726f5-7f54-40de-e989-bc1da09dd8d3","executionInfo":{"status":"ok","timestamp":1577613525664,"user_tz":-120,"elapsed":644,"user":{"displayName":"din bav","photoUrl":"","userId":"06942881549123238413"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["features_25_0.shape"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(27500, 25)"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"QE1QuYUyc4nZ","colab_type":"code","colab":{}},"source":["features_25_1=features_isFraud_1[list(df_MI['Column'].head(25))]\n","features_50_1=features_isFraud_1[list(df_MI['Column'].head(50))]\n","features_75_1=features_isFraud_1[list(df_MI['Column'].head(75))]\n","features_100_1=features_isFraud_1[list(df_MI['Column'].head(100))]\n","features_150_1=features_isFraud_1[list(df_MI['Column'].head(150))]\n","features_200_1=features_isFraud_1[list(df_MI['Column'].head(200))]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IfrlxRfydVEo","colab_type":"code","outputId":"bcb8b977-6a34-4a5b-82ef-599f60db6611","executionInfo":{"status":"ok","timestamp":1577603951257,"user_tz":-120,"elapsed":591,"user":{"displayName":"din bav","photoUrl":"","userId":"06942881549123238413"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["features_25_1.shape"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1000, 25)"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"HwnxVvVnGDa6","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mX1o0scEcVj9","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import KFold"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HXAwPZItbhWC","colab_type":"code","colab":{}},"source":["def get_kfolds(splits,features,target):\n","  kf = KFold(n_splits=splits)\n","\n","  X=features\n","  Y=target\n","\n","  X_train_base=[]\n","  X_test_base=[]\n","  Y_train_base=[]\n","  Y_test_base=[]\n","\n","  for train_index, test_index in kf.split(X):\n","      #print('train_index', train_index)\n","      #print('test_index', test_index)\n","      X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n","      Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n","      X_train_base.append(X_train)\n","      X_test_base.append(X_test)\n","      Y_train_base.append(Y_train)\n","      Y_test_base.append(Y_test)\n","  \n","  return(X_train_base,X_test_base,Y_train_base,Y_test_base)\n","  \n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NOtcY4pax_k6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"4a39a252-ff81-46cb-cba6-cc2b263c5499","executionInfo":{"status":"ok","timestamp":1577609047779,"user_tz":-120,"elapsed":4551,"user":{"displayName":"din bav","photoUrl":"","userId":"06942881549123238413"}}},"source":["to_dummy=d[cat_cols].nunique()[d[cat_cols].nunique()<10].index\n","len(to_dummy)"],"execution_count":82,"outputs":[{"output_type":"execute_result","data":{"text/plain":["29"]},"metadata":{"tags":[]},"execution_count":82}]},{"cell_type":"code","metadata":{"id":"IZ1NQ6IZfu4G","colab_type":"code","colab":{}},"source":["MI_TOP_0=[features_25_0,features_50_0,features_75_0,features_100_0,features_150_0,features_200_0]#,features_100_0,features_150_0,features_200_0\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KdqoY1B0jl4W","colab_type":"code","colab":{}},"source":["MI_TOP_1=[features_25_1,features_50_1,features_75_1,features_100_1,features_150_1,features_200_1]#,features_100_1,features_150_1,features_200_1\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"k-_cBHbL0UJI","colab_type":"code","outputId":"c1853e34-aa79-4dde-b3ec-0defcd9ff198","executionInfo":{"status":"ok","timestamp":1577613615208,"user_tz":-120,"elapsed":6326,"user":{"displayName":"din bav","photoUrl":"","userId":"06942881549123238413"}},"colab":{"base_uri":"https://localhost:8080/","height":478}},"source":["for i in range(len(MI_TOP_0)):\n","  print(MI_TOP_0[i].shape)\n","  category_cols=MI_TOP_0[i].select_dtypes(include=['object']).columns\n","  #category_cols=category_c.intersection(to_dummy)\n","  #to_drop=list(set(category_c) ^ set(category_cols))\n","  print(category_cols)\n","  MI_TOP_0[i] = pd.get_dummies(MI_TOP_0[i], columns=category_cols,drop_first=True)\n","  #MI_TOP_0[i].drop(MI_TOP_0[i].columns[to_drop], axis = 1, inplace = True)\n","  MI_TOP_1[i] = pd.get_dummies(MI_TOP_1[i], columns=category_cols,drop_first=True)\n","  #MI_TOP_1[i].drop(MI_TOP_1[i].columns[to_drop], axis = 1, inplace = True)\n","  print(MI_TOP_0[i].shape)\n"," "],"execution_count":21,"outputs":[{"output_type":"stream","text":["(27500, 25)\n","Index([], dtype='object')\n","(27500, 25)\n","(27500, 50)\n","Index([], dtype='object')\n","(27500, 50)\n","(27500, 75)\n","Index(['card1', 'card2'], dtype='object')\n","(27500, 4860)\n","(27500, 100)\n","Index(['card1', 'card2', 'id_19', 'id_20', 'id_17', 'addr1',\n","       'R_emaildomain_bin'],\n","      dtype='object')\n","(27500, 5494)\n","(27500, 150)\n","Index(['card1', 'card2', 'id_19', 'id_20', 'id_17', 'addr1',\n","       'R_emaildomain_bin', 'ProductCD', 'card3', 'addr2', 'id_13',\n","       'device_name'],\n","      dtype='object')\n","(27500, 5794)\n","(27500, 200)\n","Index(['card1', 'card2', 'id_19', 'id_20', 'id_17', 'addr1',\n","       'R_emaildomain_bin', 'ProductCD', 'card3', 'addr2', 'id_13',\n","       'device_name', 'id_28', 'id_12', 'id_37', 'card5', 'id_38'],\n","      dtype='object')\n","(27500, 5903)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X85fn6ypn4Og","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":230},"outputId":"64fcfe1d-b139-4e4a-d639-4f1c0d99935d","executionInfo":{"status":"ok","timestamp":1577609177959,"user_tz":-120,"elapsed":849,"user":{"displayName":"din bav","photoUrl":"","userId":"06942881549123238413"}}},"source":["for i in range(len(MI_TOP_0)):\n","  print(MI_TOP_0[i].isnull().sum().sum())\n","  print(MI_TOP_1[i].isnull().sum().sum())"],"execution_count":92,"outputs":[{"output_type":"stream","text":["0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GekBvmJM7MP4","colab_type":"code","outputId":"e4ea61a5-3dec-4c77-e9a3-b9ab374ae412","executionInfo":{"status":"ok","timestamp":1577609541558,"user_tz":-120,"elapsed":887,"user":{"displayName":"din bav","photoUrl":"","userId":"06942881549123238413"}},"colab":{"base_uri":"https://localhost:8080/","height":107}},"source":["train_features=[]\n","train_target=[]\n","test_features=[]\n","test_target=[]\n","i=0\n","X_train_0,X_test_0,Y_train_0,Y_test_0=get_kfolds(2,MI_TOP_0[i],target_isFraud_0)\n","X_train_1,X_test_1,Y_train_1,Y_test_1=get_kfolds(2,MI_TOP_1[i],target_isFraud_1)\n","for j in range(len(X_train_0)):\n","  tr_features = pd.concat([pd.DataFrame(X_train_0[j]), pd.DataFrame(X_train_1[j])], axis=0)\n","  tr_target = pd.concat([pd.DataFrame(Y_train_0[j]), pd.DataFrame(Y_train_1[j])], axis=0)\n","  te_features = pd.concat([pd.DataFrame(X_test_0[j]), pd.DataFrame(X_test_1[j])], axis=0)\n","  te_target = pd.concat([pd.DataFrame(Y_test_0[j]), pd.DataFrame(Y_test_1[j])], axis=0)\n","  X = np.array(tr_features)\n","  y = np.array(tr_target)\n","  sm = SMOTE(random_state=2)\n","  X_res, y_res = sm.fit_sample(X, y)\n","  #print(\"Number transactions X dataset: \", X.shape)\n","  #print(\"Number transactions y dataset: \", y.shape)\n","  #print(\"Number transactions X_res dataset: \", X_res.shape)\n","  #print(\"Number transactions y_res dataset: \", y_res.shape)\n","  #print(\"Before OverSampling, counts of label '1': {}\".format(sum(y==1)))\n","  #print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y==0)))\n","  #print(\"After OverSampling, counts of label '1': {}\".format(sum(y_res==1)))\n","  #print(\"After OverSampling, counts of label '0': {} \\n\".format(sum(y_res==0)))\n","  train_features.append(X_res)\n","  train_target.append(y_res)\n","  X_te = np.array(te_features)\n","  y_te = np.array(te_target)\n","\n","  \n","  test_features.append(X_te)\n","  test_target.append(y_te)"],"execution_count":98,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"d9Vb-D6S7MHm","colab_type":"code","outputId":"d1125149-3843-43b4-968f-9d1516082486","executionInfo":{"status":"ok","timestamp":1577609546092,"user_tz":-120,"elapsed":1384,"user":{"displayName":"din bav","photoUrl":"","userId":"06942881549123238413"}},"colab":{"base_uri":"https://localhost:8080/","height":605}},"source":["model = LogisticRegression()  # Choose a model here\n","for i in range(len(train_features)):\n","   model.fit(train_features[i], train_target[i] )\n","   y_pred = model.predict(test_features[i])\n","   print(f'Number of TOP MI: {train_features[i].shape[1]}')\n","   print(f'Number fold: {i}')\n","\n","   f_score=f1_score(test_target[i], y_pred)\n","   cm = metrics.confusion_matrix(test_target[i], y_pred)\n","   print(cm)\n","\n","   print(f'f-score: {f_score}')\n","\n","   print(classification_report(test_target[i], y_pred))"],"execution_count":99,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Number of TOP MI: 25\n","Number fold: 0\n","[[10022  3728]\n"," [  196   304]]\n","f-score: 0.13415710503089143\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.73      0.84     13750\n","           1       0.08      0.61      0.13       500\n","\n","    accuracy                           0.72     14250\n","   macro avg       0.53      0.67      0.49     14250\n","weighted avg       0.95      0.72      0.81     14250\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Number of TOP MI: 25\n","Number fold: 1\n","[[10222  3528]\n"," [  214   286]]\n","f-score: 0.13259156235512284\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.74      0.85     13750\n","           1       0.07      0.57      0.13       500\n","\n","    accuracy                           0.74     14250\n","   macro avg       0.53      0.66      0.49     14250\n","weighted avg       0.95      0.74      0.82     14250\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CMjbOJUm7k8m","outputId":"113f6d02-e219-4f26-e13a-79a34109d91e","executionInfo":{"status":"ok","timestamp":1577606826258,"user_tz":-120,"elapsed":849,"user":{"displayName":"din bav","photoUrl":"","userId":"06942881549123238413"}},"colab":{"base_uri":"https://localhost:8080/","height":107}},"source":["train_features=[]\n","train_target=[]\n","test_features=[]\n","test_target=[]\n","i=1\n","X_train_0,X_test_0,Y_train_0,Y_test_0=get_kfolds(2,MI_TOP_0[i],target_isFraud_0)\n","X_train_1,X_test_1,Y_train_1,Y_test_1=get_kfolds(2,MI_TOP_1[i],target_isFraud_1)\n","for j in range(len(X_train_0)):\n","  tr_features = pd.concat([pd.DataFrame(X_train_0[j]), pd.DataFrame(X_train_1[j])], axis=0)\n","  tr_target = pd.concat([pd.DataFrame(Y_train_0[j]), pd.DataFrame(Y_train_1[j])], axis=0)\n","  te_features = pd.concat([pd.DataFrame(X_test_0[j]), pd.DataFrame(X_test_1[j])], axis=0)\n","  te_target = pd.concat([pd.DataFrame(Y_test_0[j]), pd.DataFrame(Y_test_1[j])], axis=0)\n","  X = np.array(tr_features)\n","  y = np.array(tr_target)\n","  sm = SMOTE(random_state=2)\n","  X_res, y_res = sm.fit_sample(X, y)\n","  #print(\"Number transactions X dataset: \", X.shape)\n","  #print(\"Number transactions y dataset: \", y.shape)\n","  #print(\"Number transactions X_res dataset: \", X_res.shape)\n","  #print(\"Number transactions y_res dataset: \", y_res.shape)\n","  #print(\"Before OverSampling, counts of label '1': {}\".format(sum(y==1)))\n","  #print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y==0)))\n","  #print(\"After OverSampling, counts of label '1': {}\".format(sum(y_res==1)))\n","  #print(\"After OverSampling, counts of label '0': {} \\n\".format(sum(y_res==0)))\n","  train_features.append(X_res)\n","  train_target.append(y_res)\n","  X_te = np.array(te_features)\n","  y_te = np.array(te_target)\n","\n","  \n","  test_features.append(X_te)\n","  test_target.append(y_te)"],"execution_count":45,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bre1HjSG7k84","outputId":"b98fe5a5-f00e-4211-fc03-13967f022499","executionInfo":{"status":"ok","timestamp":1577606832661,"user_tz":-120,"elapsed":4307,"user":{"displayName":"din bav","photoUrl":"","userId":"06942881549123238413"}},"colab":{"base_uri":"https://localhost:8080/","height":534}},"source":["model = LogisticRegression()  # Choose a model here\n","for i in range(len(train_features)):\n","   model.fit(train_features[i], train_target[i] )\n","   y_pred = model.predict(test_features[i])\n","   print(f'Number of TOP MI: {train_features[i].shape[1]}')\n","   print(f'Number fold: {i}')\n","\n","   f_score=f1_score(test_target[i], y_pred)\n","   #cm = metrics.confusion_matrix(test_target[i], y_pred)\n","   #print(cm)\n","\n","   print(f'f-score: {f_score}')\n","\n","   print(classification_report(test_target[i], y_pred))"],"execution_count":46,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Number of TOP MI: 50\n","Number fold: 0\n","f-score: 0.13897550111358575\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.73      0.84     13750\n","           1       0.08      0.62      0.14       500\n","\n","    accuracy                           0.73     14250\n","   macro avg       0.53      0.68      0.49     14250\n","weighted avg       0.95      0.73      0.81     14250\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Number of TOP MI: 50\n","Number fold: 1\n","f-score: 0.13539470602014522\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.75      0.85     13750\n","           1       0.08      0.58      0.14       500\n","\n","    accuracy                           0.74     14250\n","   macro avg       0.53      0.66      0.49     14250\n","weighted avg       0.95      0.74      0.82     14250\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DQpyf5ht8JZ_","outputId":"eda889c3-6a2f-49d6-83cb-940b599e6858","executionInfo":{"status":"error","timestamp":1577609558961,"user_tz":-120,"elapsed":670,"user":{"displayName":"din bav","photoUrl":"","userId":"06942881549123238413"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["train_features=[]\n","train_target=[]\n","test_features=[]\n","test_target=[]\n","i=2\n","print(MI_TOP_0[i].info())\n","X_train_0,X_test_0,Y_train_0,Y_test_0=get_kfolds(2,MI_TOP_0[i],target_isFraud_0)\n","X_train_1,X_test_1,Y_train_1,Y_test_1=get_kfolds(2,MI_TOP_1[i],target_isFraud_1)\n","for j in range(len(X_train_0)):\n","  tr_features = pd.concat([pd.DataFrame(X_train_0[j]), pd.DataFrame(X_train_1[j])], axis=0)\n","  tr_target = pd.concat([pd.DataFrame(Y_train_0[j]), pd.DataFrame(Y_train_1[j])], axis=0)\n","  te_features = pd.concat([pd.DataFrame(X_test_0[j]), pd.DataFrame(X_test_1[j])], axis=0)\n","  te_target = pd.concat([pd.DataFrame(Y_test_0[j]), pd.DataFrame(Y_test_1[j])], axis=0)\n","  \n","  y = np.array(tr_target)\n","  X = np.array(tr_features)\n","  X=np.any(np.isnan(X))\n","  X=np.all(np.isfinite(X))\n","  X=X.reshape(-1, 1)\n","  sm = SMOTE(random_state=2)\n","  X_res, y_res = sm.fit_sample(X, y)\n","  #print(\"Number transactions X dataset: \", X.shape)\n","  #print(\"Number transactions y dataset: \", y.shape)\n","  #print(\"Number transactions X_res dataset: \", X_res.shape)\n","  #print(\"Number transactions y_res dataset: \", y_res.shape)\n","  #print(\"Before OverSampling, counts of label '1': {}\".format(sum(y==1)))\n","  #print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y==0)))\n","  #print(\"After OverSampling, counts of label '1': {}\".format(sum(y_res==1)))\n","  #print(\"After OverSampling, counts of label '0': {} \\n\".format(sum(y_res==0)))\n","  train_features.append(X_res)\n","  train_target.append(y_res)\n","  X_te = np.array(te_features)\n","  y_te = np.array(te_target)\n","\n","  \n","  test_features.append(X_te)\n","  test_target.append(y_te)"],"execution_count":100,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 27500 entries, 0 to 27499\n","Data columns (total 75 columns):\n","V123              27500 non-null float64\n","V125              27500 non-null float64\n","V249              27500 non-null float64\n","V290              27500 non-null float64\n","V124              27500 non-null float64\n","V191              27500 non-null float64\n","V196              27500 non-null float64\n","V193              27500 non-null float64\n","V247              27500 non-null float64\n","V192              27500 non-null float64\n","V23               27500 non-null float64\n","V86               27500 non-null float64\n","V26               27500 non-null float64\n","V292              27500 non-null float64\n","V188              27500 non-null float64\n","V189              27500 non-null float64\n","V252              27500 non-null float64\n","V244              27500 non-null float64\n","V186              27500 non-null float64\n","V242              27500 non-null float64\n","V200              27500 non-null float64\n","V190              27500 non-null float64\n","V201              27500 non-null float64\n","V87               27500 non-null float64\n","V246              27500 non-null float64\n","V24               27500 non-null float64\n","V176              27500 non-null float64\n","V228              27500 non-null float64\n","V257              27500 non-null float64\n","V77               27500 non-null float64\n","V199              27500 non-null float64\n","V62               27500 non-null float64\n","V20               27500 non-null float64\n","V44               27500 non-null float64\n","V245              27500 non-null float64\n","V55               27500 non-null float64\n","V45               27500 non-null float64\n","V254              27500 non-null float64\n","V78               27500 non-null float64\n","V248              27500 non-null float64\n","V56               27500 non-null float64\n","V259              27500 non-null float64\n","V6                27500 non-null float64\n","V230              27500 non-null float64\n","V3                27500 non-null float64\n","V7                27500 non-null float64\n","V170              27500 non-null float64\n","V187              27500 non-null float64\n","V37               27500 non-null float64\n","id_06             27500 non-null float64\n","V38               27500 non-null float64\n","V291              27500 non-null float64\n","V171              27500 non-null float64\n","V258              27500 non-null float64\n","V221              27500 non-null float64\n","V243              27500 non-null float64\n","id_05             27500 non-null float64\n","V229              27500 non-null float64\n","V222              27500 non-null float64\n","card1             27500 non-null object\n","V282              27500 non-null float64\n","V253              27500 non-null float64\n","V283              27500 non-null float64\n","id_01             27500 non-null float64\n","TransactionAmt    27500 non-null float64\n","C4                27500 non-null float64\n","C12               27500 non-null float64\n","C8                27500 non-null float64\n","card2             27500 non-null object\n","C7                27500 non-null float64\n","C10               27500 non-null float64\n","V256              27500 non-null float64\n","V251              27500 non-null float64\n","V255              27500 non-null float64\n","V262              27500 non-null float64\n","dtypes: float64(73), object(2)\n","memory usage: 15.9+ MB\n","None\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-100-6b150712c258>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m   \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"]}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tSRI_8sA8JaK","colab":{}},"source":["model = LogisticRegression()  # Choose a model here\n","for i in range(len(train_features)):\n","   model.fit(train_features[i], train_target[i] )\n","   y_pred = model.predict(test_features[i])\n","   print(f'Number of TOP MI: {train_features[i].shape[1]}')\n","   print(f'Number fold: {i}')\n","\n","   f_score=f1_score(test_target[i], y_pred)\n","   #cm = metrics.confusion_matrix(test_target[i], y_pred)\n","   #print(cm)\n","\n","   print(f'f-score: {f_score}')\n","\n","   print(classification_report(test_target[i], y_pred))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lcWhLQjC0-ep","colab_type":"code","outputId":"3a2dbfd2-50e6-4ec5-9735-a7bbfdefe87c","executionInfo":{"status":"ok","timestamp":1577358621564,"user_tz":-120,"elapsed":1131,"user":{"displayName":"din bav","photoUrl":"","userId":"06942881549123238413"}},"colab":{"base_uri":"https://localhost:8080/","height":433}},"source":["for i in range(len(MI_TOP_0)):\n","  print(MI_TOP_0[i].head())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["   V123  V125  V249  V290  V124  V191  ...  V242  V200  V190  V201  V87  V246\n","0   1.0   1.0  21.0   1.0   1.0   4.0  ...   2.0   2.0   2.0   2.0  1.0   2.0\n","1   1.0   1.0  21.0   1.0   1.0   4.0  ...   2.0   2.0   2.0   2.0  1.0   2.0\n","2   1.0   1.0  21.0   2.0   1.0   4.0  ...   2.0   2.0   2.0   2.0  1.0   2.0\n","3   1.0   1.0  21.0   1.0   1.0   4.0  ...   2.0   2.0   4.0   2.0  1.0   2.0\n","4   1.0   1.0  21.0   1.0   1.0   4.0  ...   2.0   2.0   2.0   1.0  1.0   2.0\n","\n","[5 rows x 25 columns]\n","   V123  V125  V249  V290  V124  V191  ...   V3   V7  V170  V187  V37  id_06\n","0   1.0   1.0  21.0   1.0   1.0   4.0  ...  2.0  2.0   1.0   2.0  2.0  -89.0\n","1   1.0   1.0  21.0   1.0   1.0   4.0  ...  1.0  1.0   4.0   2.0  1.0  -89.0\n","2   1.0   1.0  21.0   2.0   1.0   4.0  ...  2.0  1.0   4.0   2.0  2.0  -61.0\n","3   1.0   1.0  21.0   1.0   1.0   4.0  ...  1.0  1.0   4.0   2.0  1.0  -89.0\n","4   1.0   1.0  21.0   1.0   1.0   4.0  ...  1.0  1.0   4.0   2.0  1.0  -89.0\n","\n","[5 rows x 50 columns]\n","   V123  V125  V249  V290  ...  card2_597.0  card2_598.0  card2_599.0  card2_600.0\n","0   1.0   1.0  21.0   1.0  ...            0            0            0            0\n","1   1.0   1.0  21.0   1.0  ...            0            0            0            0\n","2   1.0   1.0  21.0   2.0  ...            0            0            0            0\n","3   1.0   1.0  21.0   1.0  ...            0            0            0            0\n","4   1.0   1.0  21.0   1.0  ...            0            0            0            0\n","\n","[5 rows x 9876 columns]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VrohJnz66Xn_","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q2jcrcd4gKO2","colab_type":"code","outputId":"52ac9f5a-99a2-49ff-f040-0463ac983483","executionInfo":{"status":"ok","timestamp":1576504491244,"user_tz":-120,"elapsed":22772,"user":{"displayName":"din bav","photoUrl":"","userId":"06942881549123238413"}},"colab":{"base_uri":"https://localhost:8080/","height":314}},"source":["train_features=[]\n","train_target=[]\n","test_features=[]\n","test_target=[]\n","for i in range(len(MI_TOP_0)):\n","  X_train_0,X_test_0,Y_train_0,Y_test_0=get_kfolds(2,MI_TOP_0[i],target_isFraud_0)\n","  X_train_1,X_test_1,Y_train_1,Y_test_1=get_kfolds(2,MI_TOP_1[i],target_isFraud_1)\n","  for j in range(len(X_train_0)):\n","    tr_features = pd.concat([pd.DataFrame(X_train_0[j]), pd.DataFrame(X_train_1[j])], axis=0)\n","    tr_target = pd.concat([pd.DataFrame(Y_train_0[j]), pd.DataFrame(Y_train_1[j])], axis=0)\n","    te_features = pd.concat([pd.DataFrame(X_test_0[j]), pd.DataFrame(X_test_1[j])], axis=0)\n","    te_target = pd.concat([pd.DataFrame(Y_test_0[j]), pd.DataFrame(Y_test_1[j])], axis=0)\n","    X = np.array(tr_features)\n","    y = np.array(tr_target)\n","    sm = SMOTE(random_state=2)\n","    X_res, y_res = sm.fit_sample(X, y)\n","    #print(\"Number transactions X dataset: \", X.shape)\n","    #print(\"Number transactions y dataset: \", y.shape)\n","    #print(\"Number transactions X_res dataset: \", X_res.shape)\n","    #print(\"Number transactions y_res dataset: \", y_res.shape)\n","    #print(\"Before OverSampling, counts of label '1': {}\".format(sum(y==1)))\n","    #print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y==0)))\n","    #print(\"After OverSampling, counts of label '1': {}\".format(sum(y_res==1)))\n","    #print(\"After OverSampling, counts of label '0': {} \\n\".format(sum(y_res==0)))\n","    train_features.append(X_res)\n","    train_target.append(y_res)\n","    X_te = np.array(te_features)\n","    y_te = np.array(te_target)\n","\n","    \n","    test_features.append(X_te)\n","    test_target.append(y_te)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n","of pandas will change to not sort by default.\n","\n","To accept the future behavior, pass 'sort=False'.\n","\n","To retain the current behavior and silence the warning, pass 'sort=True'.\n","\n","  if __name__ == '__main__':\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"2cO_iz-__5Bt","colab_type":"code","outputId":"2d1e23fa-7bab-4cd3-fad4-dfdc3bbf073f","executionInfo":{"status":"ok","timestamp":1576506185551,"user_tz":-120,"elapsed":1392784,"user":{"displayName":"din bav","photoUrl":"","userId":"06942881549123238413"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model = LogisticRegression()  # Choose a model here\n","for i in range(len(train_features)):\n","   model.fit(train_features[i], train_target[i] )\n","   y_pred = model.predict(test_features[i])\n","   print(f'Number of TOP MI: {train_features[i].shape[1]}')\n","   print(f'Number fold: {i}')\n","\n","   f_score=f1_score(test_target[i], y_pred)\n","   cm = metrics.confusion_matrix(test_target[i], y_pred)\n","   print(cm)\n","\n","   print(f'f-score: {f_score}')\n","\n","   print(classification_report(test_target[i], y_pred))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Number of TOP MI: 25\n","Number fold: 0\n","[[62770 22712]\n"," [ 1290  1810]]\n","f-score: 0.13105495619433782\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.73      0.84     85482\n","           1       0.07      0.58      0.13      3100\n","\n","    accuracy                           0.73     88582\n","   macro avg       0.53      0.66      0.49     88582\n","weighted avg       0.95      0.73      0.81     88582\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Number of TOP MI: 25\n","Number fold: 1\n","[[62868 22614]\n"," [ 1329  1770]]\n","f-score: 0.12880689881017354\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.74      0.84     85482\n","           1       0.07      0.57      0.13      3099\n","\n","    accuracy                           0.73     88581\n","   macro avg       0.53      0.65      0.48     88581\n","weighted avg       0.95      0.73      0.82     88581\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Number of TOP MI: 50\n","Number fold: 2\n","[[63237 22245]\n"," [ 1228  1872]]\n","f-score: 0.13756108314656282\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.74      0.84     85482\n","           1       0.08      0.60      0.14      3100\n","\n","    accuracy                           0.74     88582\n","   macro avg       0.53      0.67      0.49     88582\n","weighted avg       0.95      0.74      0.82     88582\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Number of TOP MI: 50\n","Number fold: 3\n","[[63291 22191]\n"," [ 1241  1858]]\n","f-score: 0.1368793281273022\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.74      0.84     85482\n","           1       0.08      0.60      0.14      3099\n","\n","    accuracy                           0.74     88581\n","   macro avg       0.53      0.67      0.49     88581\n","weighted avg       0.95      0.74      0.82     88581\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Number of TOP MI: 75\n","Number fold: 4\n","[[66704 18778]\n"," [ 1187  1913]]\n","f-score: 0.16081711571602705\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.78      0.87     85482\n","           1       0.09      0.62      0.16      3100\n","\n","    accuracy                           0.77     88582\n","   macro avg       0.54      0.70      0.52     88582\n","weighted avg       0.95      0.77      0.85     88582\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Number of TOP MI: 75\n","Number fold: 5\n","[[65574 19908]\n"," [ 1137  1962]]\n","f-score: 0.15715487204133124\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.77      0.86     85482\n","           1       0.09      0.63      0.16      3099\n","\n","    accuracy                           0.76     88581\n","   macro avg       0.54      0.70      0.51     88581\n","weighted avg       0.95      0.76      0.84     88581\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Number of TOP MI: 100\n","Number fold: 6\n","[[69374 16108]\n"," [ 1068  2032]]\n","f-score: 0.1913370998116761\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.81      0.89     85482\n","           1       0.11      0.66      0.19      3100\n","\n","    accuracy                           0.81     88582\n","   macro avg       0.55      0.73      0.54     88582\n","weighted avg       0.95      0.81      0.87     88582\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Number of TOP MI: 100\n","Number fold: 7\n","[[69591 15891]\n"," [ 1101  1998]]\n","f-score: 0.19039451114922812\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.81      0.89     85482\n","           1       0.11      0.64      0.19      3099\n","\n","    accuracy                           0.81     88581\n","   macro avg       0.55      0.73      0.54     88581\n","weighted avg       0.95      0.81      0.87     88581\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Number of TOP MI: 150\n","Number fold: 8\n","[[69887 15595]\n"," [ 1070  2030]]\n","f-score: 0.1958986731001206\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.82      0.89     85482\n","           1       0.12      0.65      0.20      3100\n","\n","    accuracy                           0.81     88582\n","   macro avg       0.55      0.74      0.54     88582\n","weighted avg       0.95      0.81      0.87     88582\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Number of TOP MI: 150\n","Number fold: 9\n","[[69537 15945]\n"," [ 1073  2026]]\n","f-score: 0.19231134314190793\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.81      0.89     85482\n","           1       0.11      0.65      0.19      3099\n","\n","    accuracy                           0.81     88581\n","   macro avg       0.55      0.73      0.54     88581\n","weighted avg       0.95      0.81      0.87     88581\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Number of TOP MI: 200\n","Number fold: 10\n","[[68071 17411]\n"," [ 1146  1954]]\n","f-score: 0.1739594925439573\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.80      0.88     85482\n","           1       0.10      0.63      0.17      3100\n","\n","    accuracy                           0.79     88582\n","   macro avg       0.54      0.71      0.53     88582\n","weighted avg       0.95      0.79      0.86     88582\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Number of TOP MI: 200\n","Number fold: 11\n","[[65513 19969]\n"," [ 1149  1950]]\n","f-score: 0.1558877608122152\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.77      0.86     85482\n","           1       0.09      0.63      0.16      3099\n","\n","    accuracy                           0.76     88581\n","   macro avg       0.54      0.70      0.51     88581\n","weighted avg       0.95      0.76      0.84     88581\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_fs9I4DaFfBM","colab_type":"text"},"source":["Testing on the real Test Data"]},{"cell_type":"code","metadata":{"id":"nxamHWi6F2a_","colab_type":"code","colab":{}},"source":["real_test=pd.read_csv('gdrive/My Drive/Colab Notebooks/Predictive Modeling/DATA/3. fraud_remove_nulls_fill_random_distribution/test-fraud_remove_nulls_fill_random_distribution.csv',compression='gzip')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QJBv_Mpb1AQ7","colab_type":"code","colab":{}},"source":["features_select=data[list(df_MI['Column'].head(150))]\n","test_features_select=real_test[list(df_MI['Column'].head(150))]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ck8qX_18zvl2","colab_type":"code","colab":{}},"source":["y_train=np.array(list(data['isFraud']))# .reshape(-1, 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FODAlF4izUOJ","colab_type":"code","colab":{}},"source":["X_train = np.array(features_select)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FUZgLc7Jb9GZ","colab_type":"code","colab":{}},"source":["sm = SMOTE(random_state=2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F47ERM-Qce0l","colab_type":"code","outputId":"369c0267-2c72-4612-8f5c-3a18ed178ccb","executionInfo":{"status":"ok","timestamp":1576528948956,"user_tz":-120,"elapsed":7861,"user":{"displayName":"din bav","photoUrl":"","userId":"06942881549123238413"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["#sm = SMOTE(random_state=12, ratio = 1.0)\n","x_train_res, y_train_res = sm.fit_sample(X_train, y_train)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"RfAa8mb5c3bd","colab_type":"code","outputId":"023fdb79-5002-40a3-9b3f-ee7b915e896f","executionInfo":{"status":"ok","timestamp":1576529040934,"user_tz":-120,"elapsed":2098,"user":{"displayName":"din bav","photoUrl":"","userId":"06942881549123238413"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\n","print(\"After OverSampling, counts of label '0': {} \\n\".format(sum(y_train_res==0)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["After OverSampling, counts of label '1': 170964\n","After OverSampling, counts of label '0': 170964 \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XRJIUmSwz5ru","colab":{}},"source":["y_real_test=np.array(list(real_test['isFraud'])) #.reshape(-1, 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6MQfnP37z5r9","colab":{}},"source":["X_real_test = np.array(test_features_select)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MBJQuQsXFNEm","colab_type":"code","colab":{}},"source":["model = LogisticRegression()  # Choose a model here\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ahpvbkIVef3Z","colab_type":"code","outputId":"df48704e-37a7-4c05-b2b0-3cfb2d8d965e","executionInfo":{"status":"ok","timestamp":1576531576845,"user_tz":-120,"elapsed":179877,"user":{"displayName":"din bav","photoUrl":"","userId":"06942881549123238413"}},"colab":{"base_uri":"https://localhost:8080/","height":190}},"source":["model.fit(x_train_res, y_train_res )"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n","                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n","                   multi_class='warn', n_jobs=None, penalty='l2',\n","                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n","                   warm_start=False)"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"IsqSupVceiFS","colab_type":"code","colab":{}},"source":["y_pred = model.predict(X_real_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7parM6Heekh-","colab_type":"code","colab":{}},"source":["f_score=f1_score(y_real_test, y_pred)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e4JCFxt4ngu-","colab_type":"code","outputId":"5c646473-6736-43a3-dbe3-f5801ec1f484","executionInfo":{"status":"ok","timestamp":1576531798782,"user_tz":-120,"elapsed":634,"user":{"displayName":"din bav","photoUrl":"","userId":"06942881549123238413"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(f'f-score: {f_score}')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["f-score: 0.19789201978920196\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UyHbs7MNCfOK","colab_type":"code","outputId":"4ee03ae4-385c-4992-81b6-5fedfbfbe930","executionInfo":{"status":"ok","timestamp":1576531848279,"user_tz":-120,"elapsed":1104,"user":{"displayName":"din bav","photoUrl":"","userId":"06942881549123238413"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["\n","cm = metrics.confusion_matrix(y_real_test, y_pred)\n","print(cm)\n","\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[139462  31502]\n"," [  2059   4140]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4DJ7jXC5najY","colab_type":"code","outputId":"2839bc9d-bba4-444c-b60e-f5ba5af89c1a","executionInfo":{"status":"ok","timestamp":1576531866237,"user_tz":-120,"elapsed":1186,"user":{"displayName":"din bav","photoUrl":"","userId":"06942881549123238413"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["print(classification_report(y_real_test, y_pred))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.99      0.82      0.89    170964\n","           1       0.12      0.67      0.20      6199\n","\n","    accuracy                           0.81    177163\n","   macro avg       0.55      0.74      0.55    177163\n","weighted avg       0.96      0.81      0.87    177163\n","\n"],"name":"stdout"}]}]}